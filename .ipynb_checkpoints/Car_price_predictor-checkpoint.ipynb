{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01bbf4d0",
   "metadata": {},
   "source": [
    "# Task 1 : \n",
    "### Installing Java, Spark3+ , and a compatible pyspark Python lib and making it work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79593e59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\spark-3.2.0-bin-hadoop3.2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing pyspark installation\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "import pyspark\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a76f2fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate spark context\n",
    "from pyspark import SparkContext, SparkConf \n",
    "from pyspark.sql import SparkSession\n",
    "conf = pyspark.SparkConf().setAppName('SparkApp').setMaster('local')\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark =SparkSession(sc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "680d905e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 8, 27, 64]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Example Test code\n",
    "numeric_val = sc.parallelize([1,2,3,4])\n",
    "numeric_val.map(lambda x: x*x*x).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "00ae7fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the spark session\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca6521e",
   "metadata": {},
   "source": [
    "# Task 2 \n",
    "### Read \"Car details v3.csv\" data with spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "fee7153a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+-------------+---------+------+-----------+------------+------------+----------+-------+----------+--------------------+-----+\n",
      "|                name|year|selling_price|km_driven|  fuel|seller_type|transmission|       owner|   mileage| engine| max_power|              torque|seats|\n",
      "+--------------------+----+-------------+---------+------+-----------+------------+------------+----------+-------+----------+--------------------+-----+\n",
      "|Maruti Swift Dzir...|2014|       450000|   145500|Diesel| Individual|      Manual| First Owner| 23.4 kmpl|1248 CC|    74 bhp|      190Nm@ 2000rpm|    5|\n",
      "|Skoda Rapid 1.5 T...|2014|       370000|   120000|Diesel| Individual|      Manual|Second Owner|21.14 kmpl|1498 CC|103.52 bhp| 250Nm@ 1500-2500rpm|    5|\n",
      "|Honda City 2017-2...|2006|       158000|   140000|Petrol| Individual|      Manual| Third Owner| 17.7 kmpl|1497 CC|    78 bhp|12.7@ 2,700(kgm@ ...|    5|\n",
      "|Hyundai i20 Sport...|2010|       225000|   127000|Diesel| Individual|      Manual| First Owner| 23.0 kmpl|1396 CC|    90 bhp|22.4 kgm at 1750-...|    5|\n",
      "|Maruti Swift VXI ...|2007|       130000|   120000|Petrol| Individual|      Manual| First Owner| 16.1 kmpl|1298 CC|  88.2 bhp|11.5@ 4,500(kgm@ ...|    5|\n",
      "+--------------------+----+-------------+---------+------+-----------+------------+------------+----------+-------+----------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"header\", \"true\").csv ('Car details v3.csv')\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "287d6cad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>year</th>\n",
       "      <th>selling_price</th>\n",
       "      <th>km_driven</th>\n",
       "      <th>fuel</th>\n",
       "      <th>seller_type</th>\n",
       "      <th>transmission</th>\n",
       "      <th>owner</th>\n",
       "      <th>mileage</th>\n",
       "      <th>engine</th>\n",
       "      <th>max_power</th>\n",
       "      <th>torque</th>\n",
       "      <th>seats</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Maruti Swift Dzire VDI</td>\n",
       "      <td>2014</td>\n",
       "      <td>450000</td>\n",
       "      <td>145500</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Manual</td>\n",
       "      <td>First Owner</td>\n",
       "      <td>23.4 kmpl</td>\n",
       "      <td>1248 CC</td>\n",
       "      <td>74 bhp</td>\n",
       "      <td>190Nm@ 2000rpm</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Skoda Rapid 1.5 TDI Ambition</td>\n",
       "      <td>2014</td>\n",
       "      <td>370000</td>\n",
       "      <td>120000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Manual</td>\n",
       "      <td>Second Owner</td>\n",
       "      <td>21.14 kmpl</td>\n",
       "      <td>1498 CC</td>\n",
       "      <td>103.52 bhp</td>\n",
       "      <td>250Nm@ 1500-2500rpm</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Honda City 2017-2020 EXi</td>\n",
       "      <td>2006</td>\n",
       "      <td>158000</td>\n",
       "      <td>140000</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Manual</td>\n",
       "      <td>Third Owner</td>\n",
       "      <td>17.7 kmpl</td>\n",
       "      <td>1497 CC</td>\n",
       "      <td>78 bhp</td>\n",
       "      <td>12.7@ 2,700(kgm@ rpm)</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hyundai i20 Sportz Diesel</td>\n",
       "      <td>2010</td>\n",
       "      <td>225000</td>\n",
       "      <td>127000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Manual</td>\n",
       "      <td>First Owner</td>\n",
       "      <td>23.0 kmpl</td>\n",
       "      <td>1396 CC</td>\n",
       "      <td>90 bhp</td>\n",
       "      <td>22.4 kgm at 1750-2750rpm</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Maruti Swift VXI BSIII</td>\n",
       "      <td>2007</td>\n",
       "      <td>130000</td>\n",
       "      <td>120000</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Manual</td>\n",
       "      <td>First Owner</td>\n",
       "      <td>16.1 kmpl</td>\n",
       "      <td>1298 CC</td>\n",
       "      <td>88.2 bhp</td>\n",
       "      <td>11.5@ 4,500(kgm@ rpm)</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Hyundai Xcent 1.2 VTVT E Plus</td>\n",
       "      <td>2017</td>\n",
       "      <td>440000</td>\n",
       "      <td>45000</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Manual</td>\n",
       "      <td>First Owner</td>\n",
       "      <td>20.14 kmpl</td>\n",
       "      <td>1197 CC</td>\n",
       "      <td>81.86 bhp</td>\n",
       "      <td>113.75nm@ 4000rpm</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Maruti Wagon R LXI DUO BSIII</td>\n",
       "      <td>2007</td>\n",
       "      <td>96000</td>\n",
       "      <td>175000</td>\n",
       "      <td>LPG</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Manual</td>\n",
       "      <td>First Owner</td>\n",
       "      <td>17.3 km/kg</td>\n",
       "      <td>1061 CC</td>\n",
       "      <td>57.5 bhp</td>\n",
       "      <td>7.8@ 4,500(kgm@ rpm)</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Maruti 800 DX BSII</td>\n",
       "      <td>2001</td>\n",
       "      <td>45000</td>\n",
       "      <td>5000</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Manual</td>\n",
       "      <td>Second Owner</td>\n",
       "      <td>16.1 kmpl</td>\n",
       "      <td>796 CC</td>\n",
       "      <td>37 bhp</td>\n",
       "      <td>59Nm@ 2500rpm</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Toyota Etios VXD</td>\n",
       "      <td>2011</td>\n",
       "      <td>350000</td>\n",
       "      <td>90000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Manual</td>\n",
       "      <td>First Owner</td>\n",
       "      <td>23.59 kmpl</td>\n",
       "      <td>1364 CC</td>\n",
       "      <td>67.1 bhp</td>\n",
       "      <td>170Nm@ 1800-2400rpm</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Ford Figo Diesel Celebration Edition</td>\n",
       "      <td>2013</td>\n",
       "      <td>200000</td>\n",
       "      <td>169000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Manual</td>\n",
       "      <td>First Owner</td>\n",
       "      <td>20.0 kmpl</td>\n",
       "      <td>1399 CC</td>\n",
       "      <td>68.1 bhp</td>\n",
       "      <td>160Nm@ 2000rpm</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   name  year selling_price km_driven    fuel  \\\n",
       "0                Maruti Swift Dzire VDI  2014        450000    145500  Diesel   \n",
       "1          Skoda Rapid 1.5 TDI Ambition  2014        370000    120000  Diesel   \n",
       "2              Honda City 2017-2020 EXi  2006        158000    140000  Petrol   \n",
       "3             Hyundai i20 Sportz Diesel  2010        225000    127000  Diesel   \n",
       "4                Maruti Swift VXI BSIII  2007        130000    120000  Petrol   \n",
       "5         Hyundai Xcent 1.2 VTVT E Plus  2017        440000     45000  Petrol   \n",
       "6          Maruti Wagon R LXI DUO BSIII  2007         96000    175000     LPG   \n",
       "7                    Maruti 800 DX BSII  2001         45000      5000  Petrol   \n",
       "8                      Toyota Etios VXD  2011        350000     90000  Diesel   \n",
       "9  Ford Figo Diesel Celebration Edition  2013        200000    169000  Diesel   \n",
       "\n",
       "  seller_type transmission         owner     mileage   engine   max_power  \\\n",
       "0  Individual       Manual   First Owner   23.4 kmpl  1248 CC      74 bhp   \n",
       "1  Individual       Manual  Second Owner  21.14 kmpl  1498 CC  103.52 bhp   \n",
       "2  Individual       Manual   Third Owner   17.7 kmpl  1497 CC      78 bhp   \n",
       "3  Individual       Manual   First Owner   23.0 kmpl  1396 CC      90 bhp   \n",
       "4  Individual       Manual   First Owner   16.1 kmpl  1298 CC    88.2 bhp   \n",
       "5  Individual       Manual   First Owner  20.14 kmpl  1197 CC   81.86 bhp   \n",
       "6  Individual       Manual   First Owner  17.3 km/kg  1061 CC    57.5 bhp   \n",
       "7  Individual       Manual  Second Owner   16.1 kmpl   796 CC      37 bhp   \n",
       "8  Individual       Manual   First Owner  23.59 kmpl  1364 CC    67.1 bhp   \n",
       "9  Individual       Manual   First Owner   20.0 kmpl  1399 CC    68.1 bhp   \n",
       "\n",
       "                     torque seats  \n",
       "0            190Nm@ 2000rpm     5  \n",
       "1       250Nm@ 1500-2500rpm     5  \n",
       "2     12.7@ 2,700(kgm@ rpm)     5  \n",
       "3  22.4 kgm at 1750-2750rpm     5  \n",
       "4     11.5@ 4,500(kgm@ rpm)     5  \n",
       "5         113.75nm@ 4000rpm     5  \n",
       "6      7.8@ 4,500(kgm@ rpm)     5  \n",
       "7             59Nm@ 2500rpm     4  \n",
       "8       170Nm@ 1800-2400rpm     5  \n",
       "9            160Nm@ 2000rpm     5  "
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d6f5aa",
   "metadata": {},
   "source": [
    "# Task 3\n",
    "### Creating a model to predict the selling price from the other variables using Sparks'mlib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "28c44b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('car_price_predictor').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "a46b6667",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-CRAL56H.mshome.net:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>car_price_predictor</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2308f429c40>"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "916e4cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- selling_price: string (nullable = true)\n",
      " |-- km_driven: string (nullable = true)\n",
      " |-- fuel: string (nullable = true)\n",
      " |-- seller_type: string (nullable = true)\n",
      " |-- transmission: string (nullable = true)\n",
      " |-- owner: string (nullable = true)\n",
      " |-- mileage: string (nullable = true)\n",
      " |-- engine: string (nullable = true)\n",
      " |-- max_power: string (nullable = true)\n",
      " |-- torque: string (nullable = true)\n",
      " |-- seats: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb49a1f",
   "metadata": {},
   "source": [
    "All the columns are string type.\n",
    "The selling_price is our target and the remainder are our features we want to predict the target with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecaf9eb5",
   "metadata": {},
   "source": [
    "Let's select some columns to see how the dataframe look like."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1d0b37",
   "metadata": {},
   "source": [
    "## Pre-processing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a07668f",
   "metadata": {},
   "source": [
    "### Drop the name and the torque column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "a6f90e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+---------+------+-----------+------------+------------+----------+-------+----------+-----+\n",
      "|year|selling_price|km_driven|  fuel|seller_type|transmission|       owner|   mileage| engine| max_power|seats|\n",
      "+----+-------------+---------+------+-----------+------------+------------+----------+-------+----------+-----+\n",
      "|2014|       450000|   145500|Diesel| Individual|      Manual| First Owner| 23.4 kmpl|1248 CC|    74 bhp|    5|\n",
      "|2014|       370000|   120000|Diesel| Individual|      Manual|Second Owner|21.14 kmpl|1498 CC|103.52 bhp|    5|\n",
      "|2006|       158000|   140000|Petrol| Individual|      Manual| Third Owner| 17.7 kmpl|1497 CC|    78 bhp|    5|\n",
      "|2010|       225000|   127000|Diesel| Individual|      Manual| First Owner| 23.0 kmpl|1396 CC|    90 bhp|    5|\n",
      "|2007|       130000|   120000|Petrol| Individual|      Manual| First Owner| 16.1 kmpl|1298 CC|  88.2 bhp|    5|\n",
      "+----+-------------+---------+------+-----------+------------+------------+----------+-------+----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "to_drop = [\"name\",\"torque\"]\n",
    "for x in to_drop:\n",
    "    df = df.drop(x)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca16485",
   "metadata": {},
   "source": [
    "### Replacing the year column by an age column (2021 - year )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "ab3d3279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+------+-----------+------------+------------+----------+-------+----------+-----+----+\n",
      "|selling_price|km_driven|  fuel|seller_type|transmission|       owner|   mileage| engine| max_power|seats| Age|\n",
      "+-------------+---------+------+-----------+------------+------------+----------+-------+----------+-----+----+\n",
      "|       450000|   145500|Diesel| Individual|      Manual| First Owner| 23.4 kmpl|1248 CC|    74 bhp|    5| 7.0|\n",
      "|       370000|   120000|Diesel| Individual|      Manual|Second Owner|21.14 kmpl|1498 CC|103.52 bhp|    5| 7.0|\n",
      "|       158000|   140000|Petrol| Individual|      Manual| Third Owner| 17.7 kmpl|1497 CC|    78 bhp|    5|15.0|\n",
      "|       225000|   127000|Diesel| Individual|      Manual| First Owner| 23.0 kmpl|1396 CC|    90 bhp|    5|11.0|\n",
      "|       130000|   120000|Petrol| Individual|      Manual| First Owner| 16.1 kmpl|1298 CC|  88.2 bhp|    5|14.0|\n",
      "+-------------+---------+------+-----------+------------+------------+----------+-------+----------+-----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn('Age', ( 2021 - df['year'] ) ).drop('year')\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ec6484",
   "metadata": {},
   "source": [
    "### removing units from mileage, engine and max_power columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "ec66ee9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+------+-----------+------------+------------+-----+----+-------------+------------+------------+\n",
      "|selling_price|km_driven|  fuel|seller_type|transmission|       owner|seats| Age|mileage_clean|engine_clean|mpower_clean|\n",
      "+-------------+---------+------+-----------+------------+------------+-----+----+-------------+------------+------------+\n",
      "|       450000|   145500|Diesel| Individual|      Manual| First Owner|    5| 7.0|         23.4|        1248|          74|\n",
      "|       370000|   120000|Diesel| Individual|      Manual|Second Owner|    5| 7.0|        21.14|        1498|      103.52|\n",
      "|       158000|   140000|Petrol| Individual|      Manual| Third Owner|    5|15.0|         17.7|        1497|          78|\n",
      "|       225000|   127000|Diesel| Individual|      Manual| First Owner|    5|11.0|         23.0|        1396|          90|\n",
      "|       130000|   120000|Petrol| Individual|      Manual| First Owner|    5|14.0|         16.1|        1298|        88.2|\n",
      "|       440000|    45000|Petrol| Individual|      Manual| First Owner|    5| 4.0|        20.14|        1197|       81.86|\n",
      "|        96000|   175000|   LPG| Individual|      Manual| First Owner|    5|14.0|         17.3|        1061|        57.5|\n",
      "|        45000|     5000|Petrol| Individual|      Manual|Second Owner|    4|20.0|         16.1|         796|          37|\n",
      "|       350000|    90000|Diesel| Individual|      Manual| First Owner|    5|10.0|        23.59|        1364|        67.1|\n",
      "|       200000|   169000|Diesel| Individual|      Manual| First Owner|    5| 8.0|         20.0|        1399|        68.1|\n",
      "+-------------+---------+------+-----------+------------+------------+-----+----+-------------+------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df= df.withColumn(\"mileage_clean\", regexp_extract(\"mileage\", \"[+-]?([0-9]*[.])?[0-9]+\", 0)).drop('mileage')\n",
    "df= df.withColumn(\"engine_clean\", regexp_extract(\"engine\", \"[+-]?([0-9]*[.])?[0-9]+\", 0)).drop('engine')\n",
    "df= df.withColumn(\"mpower_clean\", regexp_extract(\"max_power\", \"[+-]?([0-9]*[.])?[0-9]+\", 0)).drop('max_power')\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fae359",
   "metadata": {},
   "source": [
    "### Casting the numerical values (String  to Double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "60ad40de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- selling_price: string (nullable = true)\n",
      " |-- km_driven: string (nullable = true)\n",
      " |-- fuel: string (nullable = true)\n",
      " |-- seller_type: string (nullable = true)\n",
      " |-- transmission: string (nullable = true)\n",
      " |-- owner: string (nullable = true)\n",
      " |-- seats: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- mileage_clean: string (nullable = true)\n",
      " |-- engine_clean: string (nullable = true)\n",
      " |-- mpower_clean: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "e24559d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- selling_price: double (nullable = true)\n",
      " |-- km_driven: double (nullable = true)\n",
      " |-- fuel: string (nullable = true)\n",
      " |-- seller_type: string (nullable = true)\n",
      " |-- transmission: string (nullable = true)\n",
      " |-- owner: string (nullable = true)\n",
      " |-- seats: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- mileage_clean: double (nullable = true)\n",
      " |-- engine_clean: double (nullable = true)\n",
      " |-- mpower_clean: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "numCols=[\"Age\",\"selling_price\",\"km_driven\",\"mileage_clean\",\"engine_clean\",\n",
    "\"mpower_clean\"]\n",
    "for x in numCols:\n",
    "    df = df.withColumn(x,df[x].cast(DoubleType()))\n",
    "\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b870739b",
   "metadata": {},
   "source": [
    "### counting null values in the dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "4d5e081f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+----+-----------+------------+-----+-----+---+-------------+------------+------------+\n",
      "|selling_price|km_driven|fuel|seller_type|transmission|owner|seats|Age|mileage_clean|engine_clean|mpower_clean|\n",
      "+-------------+---------+----+-----------+------------+-----+-----+---+-------------+------------+------------+\n",
      "|            0|        0|   0|          0|           0|    0|  221|  0|          221|         221|         216|\n",
      "+-------------+---------+----+-----------+------------+-----+-----+---+-------------+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col,sum\n",
    "df.select(*(sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e4e221",
   "metadata": {},
   "source": [
    "### Replacing the null values with corresponding strategy\n",
    "\n",
    "mileage=mean\n",
    "engine=mean\n",
    "max_power=mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "359042e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+----+-----------+------------+-----+-----+---+-------------+------------+------------+\n",
      "|selling_price|km_driven|fuel|seller_type|transmission|owner|seats|Age|mileage_clean|engine_clean|mpower_clean|\n",
      "+-------------+---------+----+-----------+------------+-----+-----+---+-------------+------------+------------+\n",
      "|            0|        0|   0|          0|           0|    0|  221|  0|            0|           0|           0|\n",
      "+-------------+---------+----+-----------+------------+-----+-----+---+-------------+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "def fill_with_mean(this_df, exclude=set()):\n",
    "    stats = this_df.agg(*(avg(c).alias(c) for c in this_df.columns if c not in exclude))\n",
    "    return this_df.na.fill(stats.first().asDict())\n",
    "\n",
    "df = fill_with_mean(df, [\"year\", \"selling_price\", \"km_driven\", \"fuel\" , \"seller_type\" , \"transmission\",\"owner\",\"seats\"])\n",
    "\n",
    "df.select(*(sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad72aed3",
   "metadata": {},
   "source": [
    "### Dealing with seats null values  by setting all null values to be 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "0529df94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+----+-----------+------------+-----+-----+---+-------------+------------+------------+\n",
      "|selling_price|km_driven|fuel|seller_type|transmission|owner|seats|Age|mileage_clean|engine_clean|mpower_clean|\n",
      "+-------------+---------+----+-----------+------------+-----+-----+---+-------------+------------+------------+\n",
      "|            0|        0|   0|          0|           0|    0|    0|  0|            0|           0|           0|\n",
      "+-------------+---------+----+-----------+------------+-----+-----+---+-------------+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df= df.fillna(\"5\", subset=['seats'])\n",
    "df.select(*(sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "c2d06f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+------+-----------+------------+------------+-----+----+-------------+------------+------------+\n",
      "|selling_price|km_driven|  fuel|seller_type|transmission|       owner|seats| Age|mileage_clean|engine_clean|mpower_clean|\n",
      "+-------------+---------+------+-----------+------------+------------+-----+----+-------------+------------+------------+\n",
      "|     450000.0| 145500.0|Diesel| Individual|      Manual| First Owner|    5| 7.0|         23.4|      1248.0|        74.0|\n",
      "|     370000.0| 120000.0|Diesel| Individual|      Manual|Second Owner|    5| 7.0|        21.14|      1498.0|      103.52|\n",
      "|     158000.0| 140000.0|Petrol| Individual|      Manual| Third Owner|    5|15.0|         17.7|      1497.0|        78.0|\n",
      "|     225000.0| 127000.0|Diesel| Individual|      Manual| First Owner|    5|11.0|         23.0|      1396.0|        90.0|\n",
      "|     130000.0| 120000.0|Petrol| Individual|      Manual| First Owner|    5|14.0|         16.1|      1298.0|        88.2|\n",
      "|     440000.0|  45000.0|Petrol| Individual|      Manual| First Owner|    5| 4.0|        20.14|      1197.0|       81.86|\n",
      "|      96000.0| 175000.0|   LPG| Individual|      Manual| First Owner|    5|14.0|         17.3|      1061.0|        57.5|\n",
      "|      45000.0|   5000.0|Petrol| Individual|      Manual|Second Owner|    4|20.0|         16.1|       796.0|        37.0|\n",
      "|     350000.0|  90000.0|Diesel| Individual|      Manual| First Owner|    5|10.0|        23.59|      1364.0|        67.1|\n",
      "|     200000.0| 169000.0|Diesel| Individual|      Manual| First Owner|    5| 8.0|         20.0|      1399.0|        68.1|\n",
      "+-------------+---------+------+-----------+------------+------------+-----+----+-------------+------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_dataset = df\n",
    "final_dataset.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd06acfc",
   "metadata": {},
   "source": [
    "### spliting the data into train validation and test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "48e16c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the total dataset length : 8128 records\n",
      "train set length : 6577 records\n",
      "validation set length : 741 records\n",
      "test set length : 810 records\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Use randomSplit with weights and seed\n",
    "weights = [.8, .1, .1]\n",
    "seed = 42\n",
    "trainData, validationData, testData = final_dataset.randomSplit(weights, seed)\n",
    "\n",
    "print(f\"the total dataset length : {final_dataset.count()} records\")\n",
    "print(f\"train set length : {trainData.count()} records\")\n",
    "print(f\"validation set length : {validationData.count()} records\")\n",
    "print(f\"test set length : {testData.count()} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3cc98f",
   "metadata": {},
   "source": [
    "### checing the Dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "78f651c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('selling_price', 'double'),\n",
       " ('km_driven', 'double'),\n",
       " ('fuel', 'string'),\n",
       " ('seller_type', 'string'),\n",
       " ('transmission', 'string'),\n",
       " ('owner', 'string'),\n",
       " ('seats', 'string'),\n",
       " ('Age', 'double'),\n",
       " ('mileage_clean', 'double'),\n",
       " ('engine_clean', 'double'),\n",
       " ('mpower_clean', 'double')]"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainData.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "acd65a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the catogical columns are:  ['fuel', 'seller_type', 'transmission', 'owner', 'seats']\n",
      " the numerical columns are:  ['km_driven', 'Age', 'mileage_clean', 'engine_clean', 'mpower_clean']\n"
     ]
    }
   ],
   "source": [
    "catCols = [ x for (x , dataType) in trainData.dtypes if dataType==\"string\" ]\n",
    "numCols = [ x for (x , dataType) in trainData.dtypes if (dataType==\"double\") & (x !=\"selling_price\") ]\n",
    "print(f\" the catogical columns are:  {catCols}\")\n",
    "print(f\" the numerical columns are:  {numCols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48160f56",
   "metadata": {},
   "source": [
    "### Dealing with categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754af5b6",
   "metadata": {},
   "source": [
    "Let's identify the unique value for the string columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "178f8000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(seller_type='Individual'),\n",
       " Row(seller_type='Dealer'),\n",
       " Row(seller_type='Trustmark Dealer')]"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df.select('seller_type').distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "c8e7757d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(fuel='Diesel'), Row(fuel='CNG'), Row(fuel='LPG'), Row(fuel='Petrol')]"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('fuel').distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "f5755bfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(transmission='Automatic'), Row(transmission='Manual')]"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('transmission').distinct().collect() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "c8ec2347",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(owner='Third Owner'),\n",
       " Row(owner='Fourth & Above Owner'),\n",
       " Row(owner='Second Owner'),\n",
       " Row(owner='First Owner'),\n",
       " Row(owner='Test Drive Car')]"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('owner').distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "620b578e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(seats='7'),\n",
       " Row(seats='8'),\n",
       " Row(seats='5'),\n",
       " Row(seats='6'),\n",
       " Row(seats='9'),\n",
       " Row(seats='10'),\n",
       " Row(seats='4'),\n",
       " Row(seats='14'),\n",
       " Row(seats='2')]"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('seats').distinct().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7582072",
   "metadata": {},
   "source": [
    "### Using  One hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3888ab",
   "metadata": {},
   "source": [
    "counting the distinct type of our categrical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "3d380908",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "ead7b1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|count(fuel)|\n",
      "+-----------+\n",
      "|          4|\n",
      "+-----------+\n",
      "\n",
      "+------------+\n",
      "|count(seats)|\n",
      "+------------+\n",
      "|           9|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "trainData.agg(F.countDistinct('fuel')).show()\n",
    "trainData.agg(F.countDistinct('seats')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "6a695430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|seats|count|\n",
      "+-----+-----+\n",
      "|    7|  905|\n",
      "|    8|  187|\n",
      "|    5| 5233|\n",
      "|    6|   52|\n",
      "|    9|   67|\n",
      "|   10|   16|\n",
      "|    4|  114|\n",
      "|   14|    1|\n",
      "|    2|    2|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainData.groupBy('seats').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "caace2b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexer_ced32d8acac9,\n",
       " StringIndexer_be8309eabe77,\n",
       " StringIndexer_90aff99aca00,\n",
       " StringIndexer_30e7b34313ea,\n",
       " StringIndexer_04abca1574e1]"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from pyspark.ml.feature import (OneHotEncoder, StringIndexer)\n",
    "string_indexer = [ StringIndexer(inputCol = x, outputCol = x + \"_StringIndexer\", handleInvalid='skip')\n",
    "                  for x in catCols]\n",
    "string_indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "05ef4365",
   "metadata": {},
   "outputs": [],
   "source": [
    "One_Hot_Encoder = [ \n",
    "    OneHotEncoder(\n",
    "        inputCols=[f\"{x}_StringIndexer\" for x in catCols],\n",
    "        outputCols=[f\"{x}_OneHotEncoder\" for x in catCols],\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "ac424a0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[OneHotEncoder_7fcb63bc0c54]"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "One_Hot_Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4986c794",
   "metadata": {},
   "source": [
    "## Vector Assembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "1c7142fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "274c3608",
   "metadata": {},
   "outputs": [],
   "source": [
    "assemblerInput=[x for x in numCols]\n",
    "assemblerInput+=[f\"{x}_OneHotEncoder\" for x in catCols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "299f703f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['km_driven',\n",
       " 'Age',\n",
       " 'mileage_clean',\n",
       " 'engine_clean',\n",
       " 'mpower_clean',\n",
       " 'fuel_OneHotEncoder',\n",
       " 'seller_type_OneHotEncoder',\n",
       " 'transmission_OneHotEncoder',\n",
       " 'owner_OneHotEncoder',\n",
       " 'seats_OneHotEncoder']"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assemblerInput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "f0eb83d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_assembler = VectorAssembler(inputCols = assemblerInput, outputCol=\"VectorAssembler_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "bc2054ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "stages  = []\n",
    "stages += string_indexer\n",
    "stages += One_Hot_Encoder\n",
    "stages += [vector_assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "aa84e1cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexer_ced32d8acac9,\n",
       " StringIndexer_be8309eabe77,\n",
       " StringIndexer_90aff99aca00,\n",
       " StringIndexer_30e7b34313ea,\n",
       " StringIndexer_04abca1574e1,\n",
       " OneHotEncoder_7fcb63bc0c54,\n",
       " VectorAssembler_68c86ff0c317]"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "cf93b447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.69 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline().setStages(stages)\n",
    "pp_tr = pipeline.fit(trainData)\n",
    "\n",
    "pp_df = pp_tr.transform(trainData)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "c12265e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+------+-----------+------------+--------------------+-----+----+------------------+-----------------+-----------------+------------------+-------------------------+--------------------------+-------------------+-------------------+------------------+-------------------------+--------------------------+-------------------+-------------------+------------------------------------------------------------------------------------------------------------------------+\n",
      "|selling_price|km_driven|fuel  |seller_type|transmission|owner               |seats|Age |mileage_clean     |engine_clean     |mpower_clean     |fuel_StringIndexer|seller_type_StringIndexer|transmission_StringIndexer|owner_StringIndexer|seats_StringIndexer|fuel_OneHotEncoder|seller_type_OneHotEncoder|transmission_OneHotEncoder|owner_OneHotEncoder|seats_OneHotEncoder|VectorAssembler_features                                                                                                |\n",
      "+-------------+---------+------+-----------+------------+--------------------+-----+----+------------------+-----------------+-----------------+------------------+-------------------------+--------------------------+-------------------+-------------------+------------------+-------------------------+--------------------------+-------------------+-------------------+------------------------------------------------------------------------------------------------------------------------+\n",
      "|29999.0      |80000.0  |Petrol|Individual |Manual      |Third Owner         |4    |24.0|16.1              |796.0            |37.0             |1.0               |0.0                      |0.0                       |2.0                |3.0                |(3,[1],[1.0])     |(2,[0],[1.0])            |(1,[0],[1.0])             |(4,[2],[1.0])      |(8,[3],[1.0])      |(23,[0,1,2,3,4,6,8,10,13,18],[80000.0,24.0,16.1,796.0,37.0,1.0,1.0,1.0,1.0,1.0])                                        |\n",
      "|30000.0      |10000.0  |Petrol|Individual |Manual      |First Owner         |5    |20.0|17.3              |993.0            |60.0             |1.0               |0.0                      |0.0                       |0.0                |0.0                |(3,[1],[1.0])     |(2,[0],[1.0])            |(1,[0],[1.0])             |(4,[0],[1.0])      |(8,[0],[1.0])      |(23,[0,1,2,3,4,6,8,10,11,15],[10000.0,20.0,17.3,993.0,60.0,1.0,1.0,1.0,1.0,1.0])                                        |\n",
      "|31000.0      |56194.0  |Petrol|Individual |Manual      |Fourth & Above Owner|4    |21.0|16.1              |796.0            |37.0             |1.0               |0.0                      |0.0                       |3.0                |3.0                |(3,[1],[1.0])     |(2,[0],[1.0])            |(1,[0],[1.0])             |(4,[3],[1.0])      |(8,[3],[1.0])      |(23,[0,1,2,3,4,6,8,10,14,18],[56194.0,21.0,16.1,796.0,37.0,1.0,1.0,1.0,1.0,1.0])                                        |\n",
      "|31504.0      |110000.0 |Petrol|Individual |Manual      |Third Owner         |4    |17.0|16.1              |796.0            |37.0             |1.0               |0.0                      |0.0                       |2.0                |3.0                |(3,[1],[1.0])     |(2,[0],[1.0])            |(1,[0],[1.0])             |(4,[2],[1.0])      |(8,[3],[1.0])      |(23,[0,1,2,3,4,6,8,10,13,18],[110000.0,17.0,16.1,796.0,37.0,1.0,1.0,1.0,1.0,1.0])                                       |\n",
      "|33351.0      |90000.0  |Petrol|Individual |Manual      |Third Owner         |5    |17.0|18.9              |998.0            |67.1             |1.0               |0.0                      |0.0                       |2.0                |0.0                |(3,[1],[1.0])     |(2,[0],[1.0])            |(1,[0],[1.0])             |(4,[2],[1.0])      |(8,[0],[1.0])      |(23,[0,1,2,3,4,6,8,10,13,15],[90000.0,17.0,18.9,998.0,67.1,1.0,1.0,1.0,1.0,1.0])                                        |\n",
      "|35000.0      |10000.0  |Petrol|Individual |Manual      |First Owner         |4    |14.0|16.1              |796.0            |37.0             |1.0               |0.0                      |0.0                       |0.0                |3.0                |(3,[1],[1.0])     |(2,[0],[1.0])            |(1,[0],[1.0])             |(4,[0],[1.0])      |(8,[3],[1.0])      |(23,[0,1,2,3,4,6,8,10,11,18],[10000.0,14.0,16.1,796.0,37.0,1.0,1.0,1.0,1.0,1.0])                                        |\n",
      "|35000.0      |40000.0  |Petrol|Individual |Manual      |Second Owner        |4    |23.0|16.1              |796.0            |37.0             |1.0               |0.0                      |0.0                       |1.0                |3.0                |(3,[1],[1.0])     |(2,[0],[1.0])            |(1,[0],[1.0])             |(4,[1],[1.0])      |(8,[3],[1.0])      |(23,[0,1,2,3,4,6,8,10,12,18],[40000.0,23.0,16.1,796.0,37.0,1.0,1.0,1.0,1.0,1.0])                                        |\n",
      "|35000.0      |75000.0  |Petrol|Individual |Manual      |Second Owner        |5    |19.0|19.418783356519516|1458.625015808777|91.51791898382159|1.0               |0.0                      |0.0                       |1.0                |0.0                |(3,[1],[1.0])     |(2,[0],[1.0])            |(1,[0],[1.0])             |(4,[1],[1.0])      |(8,[0],[1.0])      |(23,[0,1,2,3,4,6,8,10,12,15],[75000.0,19.0,19.418783356519516,1458.625015808777,91.51791898382159,1.0,1.0,1.0,1.0,1.0]) |\n",
      "|35000.0      |184000.0 |Petrol|Individual |Manual      |Second Owner        |5    |19.0|19.418783356519516|1458.625015808777|91.51791898382159|1.0               |0.0                      |0.0                       |1.0                |0.0                |(3,[1],[1.0])     |(2,[0],[1.0])            |(1,[0],[1.0])             |(4,[1],[1.0])      |(8,[0],[1.0])      |(23,[0,1,2,3,4,6,8,10,12,15],[184000.0,19.0,19.418783356519516,1458.625015808777,91.51791898382159,1.0,1.0,1.0,1.0,1.0])|\n",
      "|39000.0      |42108.0  |Petrol|Individual |Manual      |First Owner         |4    |20.0|16.1              |796.0            |37.0             |1.0               |0.0                      |0.0                       |0.0                |3.0                |(3,[1],[1.0])     |(2,[0],[1.0])            |(1,[0],[1.0])             |(4,[0],[1.0])      |(8,[3],[1.0])      |(23,[0,1,2,3,4,6,8,10,11,18],[42108.0,20.0,16.1,796.0,37.0,1.0,1.0,1.0,1.0,1.0])                                        |\n",
      "|40000.0      |40000.0  |Petrol|Individual |Manual      |Second Owner        |4    |22.0|16.1              |796.0            |37.0             |1.0               |0.0                      |0.0                       |1.0                |3.0                |(3,[1],[1.0])     |(2,[0],[1.0])            |(1,[0],[1.0])             |(4,[1],[1.0])      |(8,[3],[1.0])      |(23,[0,1,2,3,4,6,8,10,12,18],[40000.0,22.0,16.1,796.0,37.0,1.0,1.0,1.0,1.0,1.0])                                        |\n",
      "|40000.0      |50000.0  |Petrol|Individual |Manual      |Second Owner        |4    |22.0|16.1              |796.0            |37.0             |1.0               |0.0                      |0.0                       |1.0                |3.0                |(3,[1],[1.0])     |(2,[0],[1.0])            |(1,[0],[1.0])             |(4,[1],[1.0])      |(8,[3],[1.0])      |(23,[0,1,2,3,4,6,8,10,12,18],[50000.0,22.0,16.1,796.0,37.0,1.0,1.0,1.0,1.0,1.0])                                        |\n",
      "|40000.0      |57000.0  |Petrol|Individual |Manual      |Second Owner        |4    |17.0|16.1              |796.0            |37.0             |1.0               |0.0                      |0.0                       |1.0                |3.0                |(3,[1],[1.0])     |(2,[0],[1.0])            |(1,[0],[1.0])             |(4,[1],[1.0])      |(8,[3],[1.0])      |(23,[0,1,2,3,4,6,8,10,12,18],[57000.0,17.0,16.1,796.0,37.0,1.0,1.0,1.0,1.0,1.0])                                        |\n",
      "|40000.0      |70000.0  |Diesel|Individual |Manual      |First Owner         |5    |18.0|19.418783356519516|1458.625015808777|91.51791898382159|0.0               |0.0                      |0.0                       |0.0                |0.0                |(3,[0],[1.0])     |(2,[0],[1.0])            |(1,[0],[1.0])             |(4,[0],[1.0])      |(8,[0],[1.0])      |(23,[0,1,2,3,4,5,8,10,11,15],[70000.0,18.0,19.418783356519516,1458.625015808777,91.51791898382159,1.0,1.0,1.0,1.0,1.0]) |\n",
      "|40000.0      |70000.0  |Petrol|Individual |Manual      |Second Owner        |4    |23.0|16.1              |796.0            |37.0             |1.0               |0.0                      |0.0                       |1.0                |3.0                |(3,[1],[1.0])     |(2,[0],[1.0])            |(1,[0],[1.0])             |(4,[1],[1.0])      |(8,[3],[1.0])      |(23,[0,1,2,3,4,6,8,10,12,18],[70000.0,23.0,16.1,796.0,37.0,1.0,1.0,1.0,1.0,1.0])                                        |\n",
      "|40000.0      |80000.0  |Petrol|Individual |Manual      |Third Owner         |4    |19.0|16.1              |796.0            |37.0             |1.0               |0.0                      |0.0                       |2.0                |3.0                |(3,[1],[1.0])     |(2,[0],[1.0])            |(1,[0],[1.0])             |(4,[2],[1.0])      |(8,[3],[1.0])      |(23,[0,1,2,3,4,6,8,10,13,18],[80000.0,19.0,16.1,796.0,37.0,1.0,1.0,1.0,1.0,1.0])                                        |\n",
      "|40000.0      |90000.0  |Petrol|Individual |Manual      |Second Owner        |5    |18.0|19.418783356519516|1458.625015808777|91.51791898382159|1.0               |0.0                      |0.0                       |1.0                |0.0                |(3,[1],[1.0])     |(2,[0],[1.0])            |(1,[0],[1.0])             |(4,[1],[1.0])      |(8,[0],[1.0])      |(23,[0,1,2,3,4,6,8,10,12,15],[90000.0,18.0,19.418783356519516,1458.625015808777,91.51791898382159,1.0,1.0,1.0,1.0,1.0]) |\n",
      "|40000.0      |96000.0  |Petrol|Individual |Manual      |Fourth & Above Owner|4    |16.0|16.1              |796.0            |37.0             |1.0               |0.0                      |0.0                       |3.0                |3.0                |(3,[1],[1.0])     |(2,[0],[1.0])            |(1,[0],[1.0])             |(4,[3],[1.0])      |(8,[3],[1.0])      |(23,[0,1,2,3,4,6,8,10,14,18],[96000.0,16.0,16.1,796.0,37.0,1.0,1.0,1.0,1.0,1.0])                                        |\n",
      "|40000.0      |120000.0 |Petrol|Individual |Manual      |First Owner         |4    |17.0|16.1              |796.0            |37.0             |1.0               |0.0                      |0.0                       |0.0                |3.0                |(3,[1],[1.0])     |(2,[0],[1.0])            |(1,[0],[1.0])             |(4,[0],[1.0])      |(8,[3],[1.0])      |(23,[0,1,2,3,4,6,8,10,11,18],[120000.0,17.0,16.1,796.0,37.0,1.0,1.0,1.0,1.0,1.0])                                       |\n",
      "|40000.0      |120000.0 |Petrol|Individual |Manual      |First Owner         |4    |24.0|16.1              |796.0            |37.0             |1.0               |0.0                      |0.0                       |0.0                |3.0                |(3,[1],[1.0])     |(2,[0],[1.0])            |(1,[0],[1.0])             |(4,[0],[1.0])      |(8,[3],[1.0])      |(23,[0,1,2,3,4,6,8,10,11,18],[120000.0,24.0,16.1,796.0,37.0,1.0,1.0,1.0,1.0,1.0])                                       |\n",
      "+-------------+---------+------+-----------+------------+--------------------+-----+----+------------------+-----------------+-----------------+------------------+-------------------------+--------------------------+-------------------+-------------------+------------------+-------------------------+--------------------------+-------------------+-------------------+------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pp_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "8a178506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- selling_price: double (nullable = true)\n",
      " |-- km_driven: double (nullable = true)\n",
      " |-- fuel: string (nullable = true)\n",
      " |-- seller_type: string (nullable = true)\n",
      " |-- transmission: string (nullable = true)\n",
      " |-- owner: string (nullable = true)\n",
      " |-- seats: string (nullable = false)\n",
      " |-- Age: double (nullable = false)\n",
      " |-- mileage_clean: double (nullable = false)\n",
      " |-- engine_clean: double (nullable = false)\n",
      " |-- mpower_clean: double (nullable = false)\n",
      " |-- fuel_StringIndexer: double (nullable = false)\n",
      " |-- seller_type_StringIndexer: double (nullable = false)\n",
      " |-- transmission_StringIndexer: double (nullable = false)\n",
      " |-- owner_StringIndexer: double (nullable = false)\n",
      " |-- seats_StringIndexer: double (nullable = false)\n",
      " |-- fuel_OneHotEncoder: vector (nullable = true)\n",
      " |-- seller_type_OneHotEncoder: vector (nullable = true)\n",
      " |-- transmission_OneHotEncoder: vector (nullable = true)\n",
      " |-- owner_OneHotEncoder: vector (nullable = true)\n",
      " |-- seats_OneHotEncoder: vector (nullable = true)\n",
      " |-- VectorAssembler_features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pp_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "e6350ace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>selling_price</th>\n",
       "      <th>km_driven</th>\n",
       "      <th>fuel</th>\n",
       "      <th>seller_type</th>\n",
       "      <th>transmission</th>\n",
       "      <th>owner</th>\n",
       "      <th>seats</th>\n",
       "      <th>Age</th>\n",
       "      <th>mileage_clean</th>\n",
       "      <th>engine_clean</th>\n",
       "      <th>...</th>\n",
       "      <th>seller_type_StringIndexer</th>\n",
       "      <th>transmission_StringIndexer</th>\n",
       "      <th>owner_StringIndexer</th>\n",
       "      <th>seats_StringIndexer</th>\n",
       "      <th>fuel_OneHotEncoder</th>\n",
       "      <th>seller_type_OneHotEncoder</th>\n",
       "      <th>transmission_OneHotEncoder</th>\n",
       "      <th>owner_OneHotEncoder</th>\n",
       "      <th>seats_OneHotEncoder</th>\n",
       "      <th>VectorAssembler_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29999.0</td>\n",
       "      <td>80000.0</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Manual</td>\n",
       "      <td>Third Owner</td>\n",
       "      <td>4</td>\n",
       "      <td>24.0</td>\n",
       "      <td>16.100000</td>\n",
       "      <td>796.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>(0.0, 1.0, 0.0)</td>\n",
       "      <td>(1.0, 0.0)</td>\n",
       "      <td>(1.0)</td>\n",
       "      <td>(0.0, 0.0, 1.0, 0.0)</td>\n",
       "      <td>(0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0)</td>\n",
       "      <td>(80000.0, 24.0, 16.1, 796.0, 37.0, 0.0, 1.0, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Manual</td>\n",
       "      <td>First Owner</td>\n",
       "      <td>5</td>\n",
       "      <td>20.0</td>\n",
       "      <td>17.300000</td>\n",
       "      <td>993.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(0.0, 1.0, 0.0)</td>\n",
       "      <td>(1.0, 0.0)</td>\n",
       "      <td>(1.0)</td>\n",
       "      <td>(1.0, 0.0, 0.0, 0.0)</td>\n",
       "      <td>(1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)</td>\n",
       "      <td>(10000.0, 20.0, 17.3, 993.0, 60.0, 0.0, 1.0, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31000.0</td>\n",
       "      <td>56194.0</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Manual</td>\n",
       "      <td>Fourth &amp; Above Owner</td>\n",
       "      <td>4</td>\n",
       "      <td>21.0</td>\n",
       "      <td>16.100000</td>\n",
       "      <td>796.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>(0.0, 1.0, 0.0)</td>\n",
       "      <td>(1.0, 0.0)</td>\n",
       "      <td>(1.0)</td>\n",
       "      <td>(0.0, 0.0, 0.0, 1.0)</td>\n",
       "      <td>(0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0)</td>\n",
       "      <td>(56194.0, 21.0, 16.1, 796.0, 37.0, 0.0, 1.0, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31504.0</td>\n",
       "      <td>110000.0</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Manual</td>\n",
       "      <td>Third Owner</td>\n",
       "      <td>4</td>\n",
       "      <td>17.0</td>\n",
       "      <td>16.100000</td>\n",
       "      <td>796.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>(0.0, 1.0, 0.0)</td>\n",
       "      <td>(1.0, 0.0)</td>\n",
       "      <td>(1.0)</td>\n",
       "      <td>(0.0, 0.0, 1.0, 0.0)</td>\n",
       "      <td>(0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0)</td>\n",
       "      <td>(110000.0, 17.0, 16.1, 796.0, 37.0, 0.0, 1.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33351.0</td>\n",
       "      <td>90000.0</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Manual</td>\n",
       "      <td>Third Owner</td>\n",
       "      <td>5</td>\n",
       "      <td>17.0</td>\n",
       "      <td>18.900000</td>\n",
       "      <td>998.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(0.0, 1.0, 0.0)</td>\n",
       "      <td>(1.0, 0.0)</td>\n",
       "      <td>(1.0)</td>\n",
       "      <td>(0.0, 0.0, 1.0, 0.0)</td>\n",
       "      <td>(1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)</td>\n",
       "      <td>(90000.0, 17.0, 18.9, 998.0, 67.1, 0.0, 1.0, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>35000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Manual</td>\n",
       "      <td>First Owner</td>\n",
       "      <td>4</td>\n",
       "      <td>14.0</td>\n",
       "      <td>16.100000</td>\n",
       "      <td>796.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>(0.0, 1.0, 0.0)</td>\n",
       "      <td>(1.0, 0.0)</td>\n",
       "      <td>(1.0)</td>\n",
       "      <td>(1.0, 0.0, 0.0, 0.0)</td>\n",
       "      <td>(0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0)</td>\n",
       "      <td>(10000.0, 14.0, 16.1, 796.0, 37.0, 0.0, 1.0, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>35000.0</td>\n",
       "      <td>40000.0</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Manual</td>\n",
       "      <td>Second Owner</td>\n",
       "      <td>4</td>\n",
       "      <td>23.0</td>\n",
       "      <td>16.100000</td>\n",
       "      <td>796.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>(0.0, 1.0, 0.0)</td>\n",
       "      <td>(1.0, 0.0)</td>\n",
       "      <td>(1.0)</td>\n",
       "      <td>(0.0, 1.0, 0.0, 0.0)</td>\n",
       "      <td>(0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0)</td>\n",
       "      <td>(40000.0, 23.0, 16.1, 796.0, 37.0, 0.0, 1.0, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>35000.0</td>\n",
       "      <td>75000.0</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Manual</td>\n",
       "      <td>Second Owner</td>\n",
       "      <td>5</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.418783</td>\n",
       "      <td>1458.625016</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(0.0, 1.0, 0.0)</td>\n",
       "      <td>(1.0, 0.0)</td>\n",
       "      <td>(1.0)</td>\n",
       "      <td>(0.0, 1.0, 0.0, 0.0)</td>\n",
       "      <td>(1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)</td>\n",
       "      <td>(75000.0, 19.0, 19.418783356519516, 1458.62501...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>35000.0</td>\n",
       "      <td>184000.0</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Manual</td>\n",
       "      <td>Second Owner</td>\n",
       "      <td>5</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.418783</td>\n",
       "      <td>1458.625016</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(0.0, 1.0, 0.0)</td>\n",
       "      <td>(1.0, 0.0)</td>\n",
       "      <td>(1.0)</td>\n",
       "      <td>(0.0, 1.0, 0.0, 0.0)</td>\n",
       "      <td>(1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)</td>\n",
       "      <td>(184000.0, 19.0, 19.418783356519516, 1458.6250...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>39000.0</td>\n",
       "      <td>42108.0</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Manual</td>\n",
       "      <td>First Owner</td>\n",
       "      <td>4</td>\n",
       "      <td>20.0</td>\n",
       "      <td>16.100000</td>\n",
       "      <td>796.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>(0.0, 1.0, 0.0)</td>\n",
       "      <td>(1.0, 0.0)</td>\n",
       "      <td>(1.0)</td>\n",
       "      <td>(1.0, 0.0, 0.0, 0.0)</td>\n",
       "      <td>(0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0)</td>\n",
       "      <td>(42108.0, 20.0, 16.1, 796.0, 37.0, 0.0, 1.0, 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows  22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   selling_price  km_driven    fuel seller_type transmission  \\\n",
       "0        29999.0    80000.0  Petrol  Individual       Manual   \n",
       "1        30000.0    10000.0  Petrol  Individual       Manual   \n",
       "2        31000.0    56194.0  Petrol  Individual       Manual   \n",
       "3        31504.0   110000.0  Petrol  Individual       Manual   \n",
       "4        33351.0    90000.0  Petrol  Individual       Manual   \n",
       "5        35000.0    10000.0  Petrol  Individual       Manual   \n",
       "6        35000.0    40000.0  Petrol  Individual       Manual   \n",
       "7        35000.0    75000.0  Petrol  Individual       Manual   \n",
       "8        35000.0   184000.0  Petrol  Individual       Manual   \n",
       "9        39000.0    42108.0  Petrol  Individual       Manual   \n",
       "\n",
       "                  owner seats   Age  mileage_clean  engine_clean  ...  \\\n",
       "0           Third Owner     4  24.0      16.100000    796.000000  ...   \n",
       "1           First Owner     5  20.0      17.300000    993.000000  ...   \n",
       "2  Fourth & Above Owner     4  21.0      16.100000    796.000000  ...   \n",
       "3           Third Owner     4  17.0      16.100000    796.000000  ...   \n",
       "4           Third Owner     5  17.0      18.900000    998.000000  ...   \n",
       "5           First Owner     4  14.0      16.100000    796.000000  ...   \n",
       "6          Second Owner     4  23.0      16.100000    796.000000  ...   \n",
       "7          Second Owner     5  19.0      19.418783   1458.625016  ...   \n",
       "8          Second Owner     5  19.0      19.418783   1458.625016  ...   \n",
       "9           First Owner     4  20.0      16.100000    796.000000  ...   \n",
       "\n",
       "   seller_type_StringIndexer  transmission_StringIndexer  owner_StringIndexer  \\\n",
       "0                        0.0                         0.0                  2.0   \n",
       "1                        0.0                         0.0                  0.0   \n",
       "2                        0.0                         0.0                  3.0   \n",
       "3                        0.0                         0.0                  2.0   \n",
       "4                        0.0                         0.0                  2.0   \n",
       "5                        0.0                         0.0                  0.0   \n",
       "6                        0.0                         0.0                  1.0   \n",
       "7                        0.0                         0.0                  1.0   \n",
       "8                        0.0                         0.0                  1.0   \n",
       "9                        0.0                         0.0                  0.0   \n",
       "\n",
       "   seats_StringIndexer  fuel_OneHotEncoder  seller_type_OneHotEncoder  \\\n",
       "0                  3.0     (0.0, 1.0, 0.0)                 (1.0, 0.0)   \n",
       "1                  0.0     (0.0, 1.0, 0.0)                 (1.0, 0.0)   \n",
       "2                  3.0     (0.0, 1.0, 0.0)                 (1.0, 0.0)   \n",
       "3                  3.0     (0.0, 1.0, 0.0)                 (1.0, 0.0)   \n",
       "4                  0.0     (0.0, 1.0, 0.0)                 (1.0, 0.0)   \n",
       "5                  3.0     (0.0, 1.0, 0.0)                 (1.0, 0.0)   \n",
       "6                  3.0     (0.0, 1.0, 0.0)                 (1.0, 0.0)   \n",
       "7                  0.0     (0.0, 1.0, 0.0)                 (1.0, 0.0)   \n",
       "8                  0.0     (0.0, 1.0, 0.0)                 (1.0, 0.0)   \n",
       "9                  3.0     (0.0, 1.0, 0.0)                 (1.0, 0.0)   \n",
       "\n",
       "  transmission_OneHotEncoder   owner_OneHotEncoder  \\\n",
       "0                      (1.0)  (0.0, 0.0, 1.0, 0.0)   \n",
       "1                      (1.0)  (1.0, 0.0, 0.0, 0.0)   \n",
       "2                      (1.0)  (0.0, 0.0, 0.0, 1.0)   \n",
       "3                      (1.0)  (0.0, 0.0, 1.0, 0.0)   \n",
       "4                      (1.0)  (0.0, 0.0, 1.0, 0.0)   \n",
       "5                      (1.0)  (1.0, 0.0, 0.0, 0.0)   \n",
       "6                      (1.0)  (0.0, 1.0, 0.0, 0.0)   \n",
       "7                      (1.0)  (0.0, 1.0, 0.0, 0.0)   \n",
       "8                      (1.0)  (0.0, 1.0, 0.0, 0.0)   \n",
       "9                      (1.0)  (1.0, 0.0, 0.0, 0.0)   \n",
       "\n",
       "                        seats_OneHotEncoder  \\\n",
       "0  (0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0)   \n",
       "1  (1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)   \n",
       "2  (0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0)   \n",
       "3  (0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0)   \n",
       "4  (1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)   \n",
       "5  (0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0)   \n",
       "6  (0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0)   \n",
       "7  (1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)   \n",
       "8  (1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)   \n",
       "9  (0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0)   \n",
       "\n",
       "                            VectorAssembler_features  \n",
       "0  (80000.0, 24.0, 16.1, 796.0, 37.0, 0.0, 1.0, 0...  \n",
       "1  (10000.0, 20.0, 17.3, 993.0, 60.0, 0.0, 1.0, 0...  \n",
       "2  (56194.0, 21.0, 16.1, 796.0, 37.0, 0.0, 1.0, 0...  \n",
       "3  (110000.0, 17.0, 16.1, 796.0, 37.0, 0.0, 1.0, ...  \n",
       "4  (90000.0, 17.0, 18.9, 998.0, 67.1, 0.0, 1.0, 0...  \n",
       "5  (10000.0, 14.0, 16.1, 796.0, 37.0, 0.0, 1.0, 0...  \n",
       "6  (40000.0, 23.0, 16.1, 796.0, 37.0, 0.0, 1.0, 0...  \n",
       "7  (75000.0, 19.0, 19.418783356519516, 1458.62501...  \n",
       "8  (184000.0, 19.0, 19.418783356519516, 1458.6250...  \n",
       "9  (42108.0, 20.0, 16.1, 796.0, 37.0, 0.0, 1.0, 0...  \n",
       "\n",
       "[10 rows x 22 columns]"
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp_df.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "5b75f92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+------------------+-----------------+-----------------+------------------+-------------------------+--------------------------+-------------------+-------------------+\n",
      "|km_driven|Age |mileage_clean     |engine_clean     |mpower_clean     |fuel_OneHotEncoder|seller_type_OneHotEncoder|transmission_OneHotEncoder|owner_OneHotEncoder|seats_OneHotEncoder|\n",
      "+---------+----+------------------+-----------------+-----------------+------------------+-------------------------+--------------------------+-------------------+-------------------+\n",
      "|80000.0  |24.0|16.1              |796.0            |37.0             |(3,[1],[1.0])     |(2,[0],[1.0])            |(1,[0],[1.0])             |(4,[2],[1.0])      |(8,[3],[1.0])      |\n",
      "|10000.0  |20.0|17.3              |993.0            |60.0             |(3,[1],[1.0])     |(2,[0],[1.0])            |(1,[0],[1.0])             |(4,[0],[1.0])      |(8,[0],[1.0])      |\n",
      "|56194.0  |21.0|16.1              |796.0            |37.0             |(3,[1],[1.0])     |(2,[0],[1.0])            |(1,[0],[1.0])             |(4,[3],[1.0])      |(8,[3],[1.0])      |\n",
      "|110000.0 |17.0|16.1              |796.0            |37.0             |(3,[1],[1.0])     |(2,[0],[1.0])            |(1,[0],[1.0])             |(4,[2],[1.0])      |(8,[3],[1.0])      |\n",
      "|90000.0  |17.0|18.9              |998.0            |67.1             |(3,[1],[1.0])     |(2,[0],[1.0])            |(1,[0],[1.0])             |(4,[2],[1.0])      |(8,[0],[1.0])      |\n",
      "|10000.0  |14.0|16.1              |796.0            |37.0             |(3,[1],[1.0])     |(2,[0],[1.0])            |(1,[0],[1.0])             |(4,[0],[1.0])      |(8,[3],[1.0])      |\n",
      "|40000.0  |23.0|16.1              |796.0            |37.0             |(3,[1],[1.0])     |(2,[0],[1.0])            |(1,[0],[1.0])             |(4,[1],[1.0])      |(8,[3],[1.0])      |\n",
      "|75000.0  |19.0|19.418783356519516|1458.625015808777|91.51791898382159|(3,[1],[1.0])     |(2,[0],[1.0])            |(1,[0],[1.0])             |(4,[1],[1.0])      |(8,[0],[1.0])      |\n",
      "|184000.0 |19.0|19.418783356519516|1458.625015808777|91.51791898382159|(3,[1],[1.0])     |(2,[0],[1.0])            |(1,[0],[1.0])             |(4,[1],[1.0])      |(8,[0],[1.0])      |\n",
      "|42108.0  |20.0|16.1              |796.0            |37.0             |(3,[1],[1.0])     |(2,[0],[1.0])            |(1,[0],[1.0])             |(4,[0],[1.0])      |(8,[3],[1.0])      |\n",
      "|40000.0  |22.0|16.1              |796.0            |37.0             |(3,[1],[1.0])     |(2,[0],[1.0])            |(1,[0],[1.0])             |(4,[1],[1.0])      |(8,[3],[1.0])      |\n",
      "|50000.0  |22.0|16.1              |796.0            |37.0             |(3,[1],[1.0])     |(2,[0],[1.0])            |(1,[0],[1.0])             |(4,[1],[1.0])      |(8,[3],[1.0])      |\n",
      "|57000.0  |17.0|16.1              |796.0            |37.0             |(3,[1],[1.0])     |(2,[0],[1.0])            |(1,[0],[1.0])             |(4,[1],[1.0])      |(8,[3],[1.0])      |\n",
      "|70000.0  |18.0|19.418783356519516|1458.625015808777|91.51791898382159|(3,[0],[1.0])     |(2,[0],[1.0])            |(1,[0],[1.0])             |(4,[0],[1.0])      |(8,[0],[1.0])      |\n",
      "|70000.0  |23.0|16.1              |796.0            |37.0             |(3,[1],[1.0])     |(2,[0],[1.0])            |(1,[0],[1.0])             |(4,[1],[1.0])      |(8,[3],[1.0])      |\n",
      "|80000.0  |19.0|16.1              |796.0            |37.0             |(3,[1],[1.0])     |(2,[0],[1.0])            |(1,[0],[1.0])             |(4,[2],[1.0])      |(8,[3],[1.0])      |\n",
      "|90000.0  |18.0|19.418783356519516|1458.625015808777|91.51791898382159|(3,[1],[1.0])     |(2,[0],[1.0])            |(1,[0],[1.0])             |(4,[1],[1.0])      |(8,[0],[1.0])      |\n",
      "|96000.0  |16.0|16.1              |796.0            |37.0             |(3,[1],[1.0])     |(2,[0],[1.0])            |(1,[0],[1.0])             |(4,[3],[1.0])      |(8,[3],[1.0])      |\n",
      "|120000.0 |17.0|16.1              |796.0            |37.0             |(3,[1],[1.0])     |(2,[0],[1.0])            |(1,[0],[1.0])             |(4,[0],[1.0])      |(8,[3],[1.0])      |\n",
      "|120000.0 |24.0|16.1              |796.0            |37.0             |(3,[1],[1.0])     |(2,[0],[1.0])            |(1,[0],[1.0])             |(4,[0],[1.0])      |(8,[3],[1.0])      |\n",
      "+---------+----+------------------+-----------------+-----------------+------------------+-------------------------+--------------------------+-------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pp_df.select(assemblerInput).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "86e8357d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------+\n",
      "|VectorAssembler_features                                                                                                |\n",
      "+------------------------------------------------------------------------------------------------------------------------+\n",
      "|(23,[0,1,2,3,4,6,8,10,13,18],[80000.0,24.0,16.1,796.0,37.0,1.0,1.0,1.0,1.0,1.0])                                        |\n",
      "|(23,[0,1,2,3,4,6,8,10,11,15],[10000.0,20.0,17.3,993.0,60.0,1.0,1.0,1.0,1.0,1.0])                                        |\n",
      "|(23,[0,1,2,3,4,6,8,10,14,18],[56194.0,21.0,16.1,796.0,37.0,1.0,1.0,1.0,1.0,1.0])                                        |\n",
      "|(23,[0,1,2,3,4,6,8,10,13,18],[110000.0,17.0,16.1,796.0,37.0,1.0,1.0,1.0,1.0,1.0])                                       |\n",
      "|(23,[0,1,2,3,4,6,8,10,13,15],[90000.0,17.0,18.9,998.0,67.1,1.0,1.0,1.0,1.0,1.0])                                        |\n",
      "|(23,[0,1,2,3,4,6,8,10,11,18],[10000.0,14.0,16.1,796.0,37.0,1.0,1.0,1.0,1.0,1.0])                                        |\n",
      "|(23,[0,1,2,3,4,6,8,10,12,18],[40000.0,23.0,16.1,796.0,37.0,1.0,1.0,1.0,1.0,1.0])                                        |\n",
      "|(23,[0,1,2,3,4,6,8,10,12,15],[75000.0,19.0,19.418783356519516,1458.625015808777,91.51791898382159,1.0,1.0,1.0,1.0,1.0]) |\n",
      "|(23,[0,1,2,3,4,6,8,10,12,15],[184000.0,19.0,19.418783356519516,1458.625015808777,91.51791898382159,1.0,1.0,1.0,1.0,1.0])|\n",
      "|(23,[0,1,2,3,4,6,8,10,11,18],[42108.0,20.0,16.1,796.0,37.0,1.0,1.0,1.0,1.0,1.0])                                        |\n",
      "|(23,[0,1,2,3,4,6,8,10,12,18],[40000.0,22.0,16.1,796.0,37.0,1.0,1.0,1.0,1.0,1.0])                                        |\n",
      "|(23,[0,1,2,3,4,6,8,10,12,18],[50000.0,22.0,16.1,796.0,37.0,1.0,1.0,1.0,1.0,1.0])                                        |\n",
      "|(23,[0,1,2,3,4,6,8,10,12,18],[57000.0,17.0,16.1,796.0,37.0,1.0,1.0,1.0,1.0,1.0])                                        |\n",
      "|(23,[0,1,2,3,4,5,8,10,11,15],[70000.0,18.0,19.418783356519516,1458.625015808777,91.51791898382159,1.0,1.0,1.0,1.0,1.0]) |\n",
      "|(23,[0,1,2,3,4,6,8,10,12,18],[70000.0,23.0,16.1,796.0,37.0,1.0,1.0,1.0,1.0,1.0])                                        |\n",
      "|(23,[0,1,2,3,4,6,8,10,13,18],[80000.0,19.0,16.1,796.0,37.0,1.0,1.0,1.0,1.0,1.0])                                        |\n",
      "|(23,[0,1,2,3,4,6,8,10,12,15],[90000.0,18.0,19.418783356519516,1458.625015808777,91.51791898382159,1.0,1.0,1.0,1.0,1.0]) |\n",
      "|(23,[0,1,2,3,4,6,8,10,14,18],[96000.0,16.0,16.1,796.0,37.0,1.0,1.0,1.0,1.0,1.0])                                        |\n",
      "|(23,[0,1,2,3,4,6,8,10,11,18],[120000.0,17.0,16.1,796.0,37.0,1.0,1.0,1.0,1.0,1.0])                                       |\n",
      "|(23,[0,1,2,3,4,6,8,10,11,18],[120000.0,24.0,16.1,796.0,37.0,1.0,1.0,1.0,1.0,1.0])                                       |\n",
      "+------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pp_df.select('VectorAssembler_features').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f489fe",
   "metadata": {},
   "source": [
    "# Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "fbc6cfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pp_df.select(\n",
    "F.col(\"VectorAssembler_features\").alias(\"features\"),\n",
    "F.col(\"selling_price\").alias(\"label\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "d9d484f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+--------------------+\n",
      "|            features|  label|     scaled_features|\n",
      "+--------------------+-------+--------------------+\n",
      "|(23,[0,1,2,3,4,6,...|29999.0|(23,[0,1,2,3,4,6,...|\n",
      "|(23,[0,1,2,3,4,6,...|30000.0|(23,[0,1,2,3,4,6,...|\n",
      "|(23,[0,1,2,3,4,6,...|31000.0|(23,[0,1,2,3,4,6,...|\n",
      "|(23,[0,1,2,3,4,6,...|31504.0|(23,[0,1,2,3,4,6,...|\n",
      "|(23,[0,1,2,3,4,6,...|33351.0|(23,[0,1,2,3,4,6,...|\n",
      "+--------------------+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "# Let us create an object of StandardScaler class\n",
    "Scalerizer=StandardScaler().setInputCol(\"features\").setOutputCol(\"scaled_features\")\n",
    "Scalerizer.fit(data).transform(data).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "aad75925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|  label|     scaled_features|\n",
      "+-------+--------------------+\n",
      "|29999.0|(23,[0,1,2,3,4,6,...|\n",
      "|30000.0|(23,[0,1,2,3,4,6,...|\n",
      "|31000.0|(23,[0,1,2,3,4,6,...|\n",
      "|31504.0|(23,[0,1,2,3,4,6,...|\n",
      "|33351.0|(23,[0,1,2,3,4,6,...|\n",
      "+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaled_data = Scalerizer.fit(data).transform(data).drop('features')\n",
    "scaled_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e24efd",
   "metadata": {},
   "source": [
    "#  TRAIN Random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "id": "6b1c57e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "id": "9cf3ff51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#rf = RandomForestRegressor(labelCol = \"label\", featuresCol = \"scaled_features\")\n",
    "rf = RandomForestRegressor(labelCol = \"label\", featuresCol = \"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "0912b808",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rfmodel=rf.fit(scaled_data)\n",
    "rfmodel=rf.fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c623c6c",
   "metadata": {},
   "source": [
    "# validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "id": "3d95057e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_val= pipeline.fit(validationData)\n",
    "\n",
    "pp_df2 = pp.transform(validationData)\n",
    "data2 = pp_df2.select(\n",
    "F.col(\"VectorAssembler_features\").alias(\"features\"),\n",
    "F.col(\"selling_price\").alias(\"label\"))\n",
    "Scalerizer2=StandardScaler().setInputCol(\"features\").setOutputCol(\"scaled_features\")\n",
    "scaled_data2 = Scalerizer2.fit(data2).transform(data2).drop('features')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "id": "d8b71527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|  label|     scaled_features|\n",
      "+-------+--------------------+\n",
      "|30000.0|(21,[0,1,2,3,4,6,...|\n",
      "|40000.0|(21,[0,1,2,3,4,6,...|\n",
      "|40000.0|(21,[0,1,2,3,4,6,...|\n",
      "|40000.0|(21,[0,1,2,3,4,6,...|\n",
      "|45000.0|(21,[0,1,2,3,4,6,...|\n",
      "+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaled_data2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "f535adc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred = rfmodel.transform(scaled_data2)\n",
    "pred = rfmodel.transform(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "531a5e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: vector (nullable = true)\n",
      " |-- label: double (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "c4097660",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o12357.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 743.0 failed 1 times, most recent failure: Lost task 0.0 in stage 743.0 (TID 555) (DESKTOP-CRAL56H.mshome.net executor driver): org.apache.spark.SparkException: Failed to execute user defined function (RandomForestRegressionModel$$Lambda$4544/0x00000001017d8840: (struct<type:tinyint,size:int,indices:array<int>,values:array<double>>) => double)\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:136)\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.lang.IndexOutOfBoundsException: Index 22 out of bounds [0, 21)\r\n\tat org.apache.spark.ml.linalg.SparseVector.apply(Vectors.scala:650)\r\n\tat org.apache.spark.ml.tree.CategoricalSplit.shouldGoLeft(Split.scala:99)\r\n\tat org.apache.spark.ml.tree.InternalNode.predictImpl(Node.scala:180)\r\n\tat org.apache.spark.ml.regression.RandomForestRegressionModel.$anonfun$predict$1(RandomForestRegressor.scala:259)\r\n\tat org.apache.spark.ml.regression.RandomForestRegressionModel.$anonfun$predict$1$adapted(RandomForestRegressor.scala:259)\r\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\r\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\r\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\r\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\r\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\r\n\tat org.apache.spark.ml.regression.RandomForestRegressionModel.predict(RandomForestRegressor.scala:259)\r\n\tat org.apache.spark.ml.regression.RandomForestRegressionModel.$anonfun$transform$1(RandomForestRegressor.scala:233)\r\n\tat org.apache.spark.ml.regression.RandomForestRegressionModel.$anonfun$transform$1$adapted(RandomForestRegressor.scala:233)\r\n\t... 17 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2403)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2352)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2351)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2351)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1109)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1109)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1109)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2591)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2533)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2522)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:898)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:476)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:429)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3715)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2728)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2728)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2935)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:287)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:326)\r\n\tat jdk.internal.reflect.GeneratedMethodAccessor71.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function (RandomForestRegressionModel$$Lambda$4544/0x00000001017d8840: (struct<type:tinyint,size:int,indices:array<int>,values:array<double>>) => double)\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:136)\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\nCaused by: java.lang.IndexOutOfBoundsException: Index 22 out of bounds [0, 21)\r\n\tat org.apache.spark.ml.linalg.SparseVector.apply(Vectors.scala:650)\r\n\tat org.apache.spark.ml.tree.CategoricalSplit.shouldGoLeft(Split.scala:99)\r\n\tat org.apache.spark.ml.tree.InternalNode.predictImpl(Node.scala:180)\r\n\tat org.apache.spark.ml.regression.RandomForestRegressionModel.$anonfun$predict$1(RandomForestRegressor.scala:259)\r\n\tat org.apache.spark.ml.regression.RandomForestRegressionModel.$anonfun$predict$1$adapted(RandomForestRegressor.scala:259)\r\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\r\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\r\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\r\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\r\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\r\n\tat org.apache.spark.ml.regression.RandomForestRegressionModel.predict(RandomForestRegressor.scala:259)\r\n\tat org.apache.spark.ml.regression.RandomForestRegressionModel.$anonfun$transform$1(RandomForestRegressor.scala:233)\r\n\tat org.apache.spark.ml.regression.RandomForestRegressionModel.$anonfun$transform$1$adapted(RandomForestRegressor.scala:233)\r\n\t... 17 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13120/1353913820.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\spark-3.2.0-bin-hadoop3.2\\python\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    492\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 494\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    495\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    496\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-3.2.0-bin-hadoop3.2\\python\\lib\\py4j-0.10.9.2-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1309\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1310\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-3.2.0-bin-hadoop3.2\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-3.2.0-bin-hadoop3.2\\python\\lib\\py4j-0.10.9.2-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o12357.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 743.0 failed 1 times, most recent failure: Lost task 0.0 in stage 743.0 (TID 555) (DESKTOP-CRAL56H.mshome.net executor driver): org.apache.spark.SparkException: Failed to execute user defined function (RandomForestRegressionModel$$Lambda$4544/0x00000001017d8840: (struct<type:tinyint,size:int,indices:array<int>,values:array<double>>) => double)\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:136)\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.lang.IndexOutOfBoundsException: Index 22 out of bounds [0, 21)\r\n\tat org.apache.spark.ml.linalg.SparseVector.apply(Vectors.scala:650)\r\n\tat org.apache.spark.ml.tree.CategoricalSplit.shouldGoLeft(Split.scala:99)\r\n\tat org.apache.spark.ml.tree.InternalNode.predictImpl(Node.scala:180)\r\n\tat org.apache.spark.ml.regression.RandomForestRegressionModel.$anonfun$predict$1(RandomForestRegressor.scala:259)\r\n\tat org.apache.spark.ml.regression.RandomForestRegressionModel.$anonfun$predict$1$adapted(RandomForestRegressor.scala:259)\r\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\r\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\r\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\r\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\r\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\r\n\tat org.apache.spark.ml.regression.RandomForestRegressionModel.predict(RandomForestRegressor.scala:259)\r\n\tat org.apache.spark.ml.regression.RandomForestRegressionModel.$anonfun$transform$1(RandomForestRegressor.scala:233)\r\n\tat org.apache.spark.ml.regression.RandomForestRegressionModel.$anonfun$transform$1$adapted(RandomForestRegressor.scala:233)\r\n\t... 17 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2403)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2352)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2351)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2351)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1109)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1109)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1109)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2591)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2533)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2522)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:898)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:476)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:429)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3715)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2728)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2728)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2935)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:287)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:326)\r\n\tat jdk.internal.reflect.GeneratedMethodAccessor71.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function (RandomForestRegressionModel$$Lambda$4544/0x00000001017d8840: (struct<type:tinyint,size:int,indices:array<int>,values:array<double>>) => double)\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:136)\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\nCaused by: java.lang.IndexOutOfBoundsException: Index 22 out of bounds [0, 21)\r\n\tat org.apache.spark.ml.linalg.SparseVector.apply(Vectors.scala:650)\r\n\tat org.apache.spark.ml.tree.CategoricalSplit.shouldGoLeft(Split.scala:99)\r\n\tat org.apache.spark.ml.tree.InternalNode.predictImpl(Node.scala:180)\r\n\tat org.apache.spark.ml.regression.RandomForestRegressionModel.$anonfun$predict$1(RandomForestRegressor.scala:259)\r\n\tat org.apache.spark.ml.regression.RandomForestRegressionModel.$anonfun$predict$1$adapted(RandomForestRegressor.scala:259)\r\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\r\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\r\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\r\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\r\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\r\n\tat org.apache.spark.ml.regression.RandomForestRegressionModel.predict(RandomForestRegressor.scala:259)\r\n\tat org.apache.spark.ml.regression.RandomForestRegressionModel.$anonfun$transform$1(RandomForestRegressor.scala:233)\r\n\tat org.apache.spark.ml.regression.RandomForestRegressionModel.$anonfun$transform$1$adapted(RandomForestRegressor.scala:233)\r\n\t... 17 more\r\n"
     ]
    }
   ],
   "source": [
    "pred.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "e1185d97",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o12366.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 744.0 failed 1 times, most recent failure: Lost task 0.0 in stage 744.0 (TID 556) (DESKTOP-CRAL56H.mshome.net executor driver): org.apache.spark.SparkException: Failed to execute user defined function (RandomForestRegressionModel$$Lambda$4544/0x00000001017d8840: (struct<type:tinyint,size:int,indices:array<int>,values:array<double>>) => double)\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:136)\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.lang.IndexOutOfBoundsException: Index 22 out of bounds [0, 21)\r\n\tat org.apache.spark.ml.linalg.SparseVector.apply(Vectors.scala:650)\r\n\tat org.apache.spark.ml.tree.CategoricalSplit.shouldGoLeft(Split.scala:99)\r\n\tat org.apache.spark.ml.tree.InternalNode.predictImpl(Node.scala:180)\r\n\tat org.apache.spark.ml.regression.RandomForestRegressionModel.$anonfun$predict$1(RandomForestRegressor.scala:259)\r\n\tat org.apache.spark.ml.regression.RandomForestRegressionModel.$anonfun$predict$1$adapted(RandomForestRegressor.scala:259)\r\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\r\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\r\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\r\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\r\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\r\n\tat org.apache.spark.ml.regression.RandomForestRegressionModel.predict(RandomForestRegressor.scala:259)\r\n\tat org.apache.spark.ml.regression.RandomForestRegressionModel.$anonfun$transform$1(RandomForestRegressor.scala:233)\r\n\tat org.apache.spark.ml.regression.RandomForestRegressionModel.$anonfun$transform$1$adapted(RandomForestRegressor.scala:233)\r\n\t... 17 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2403)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2352)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2351)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2351)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1109)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1109)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1109)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2591)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2533)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2522)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:898)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:476)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:429)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3715)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2728)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2728)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2935)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:287)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:326)\r\n\tat jdk.internal.reflect.GeneratedMethodAccessor71.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function (RandomForestRegressionModel$$Lambda$4544/0x00000001017d8840: (struct<type:tinyint,size:int,indices:array<int>,values:array<double>>) => double)\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:136)\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\nCaused by: java.lang.IndexOutOfBoundsException: Index 22 out of bounds [0, 21)\r\n\tat org.apache.spark.ml.linalg.SparseVector.apply(Vectors.scala:650)\r\n\tat org.apache.spark.ml.tree.CategoricalSplit.shouldGoLeft(Split.scala:99)\r\n\tat org.apache.spark.ml.tree.InternalNode.predictImpl(Node.scala:180)\r\n\tat org.apache.spark.ml.regression.RandomForestRegressionModel.$anonfun$predict$1(RandomForestRegressor.scala:259)\r\n\tat org.apache.spark.ml.regression.RandomForestRegressionModel.$anonfun$predict$1$adapted(RandomForestRegressor.scala:259)\r\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\r\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\r\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\r\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\r\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\r\n\tat org.apache.spark.ml.regression.RandomForestRegressionModel.predict(RandomForestRegressor.scala:259)\r\n\tat org.apache.spark.ml.regression.RandomForestRegressionModel.$anonfun$transform$1(RandomForestRegressor.scala:233)\r\n\tat org.apache.spark.ml.regression.RandomForestRegressionModel.$anonfun$transform$1$adapted(RandomForestRegressor.scala:233)\r\n\t... 17 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13120/1575475994.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"label\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"prediction\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\spark-3.2.0-bin-hadoop3.2\\python\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    492\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 494\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    495\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    496\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-3.2.0-bin-hadoop3.2\\python\\lib\\py4j-0.10.9.2-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1309\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1310\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-3.2.0-bin-hadoop3.2\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-3.2.0-bin-hadoop3.2\\python\\lib\\py4j-0.10.9.2-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o12366.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 744.0 failed 1 times, most recent failure: Lost task 0.0 in stage 744.0 (TID 556) (DESKTOP-CRAL56H.mshome.net executor driver): org.apache.spark.SparkException: Failed to execute user defined function (RandomForestRegressionModel$$Lambda$4544/0x00000001017d8840: (struct<type:tinyint,size:int,indices:array<int>,values:array<double>>) => double)\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:136)\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.lang.IndexOutOfBoundsException: Index 22 out of bounds [0, 21)\r\n\tat org.apache.spark.ml.linalg.SparseVector.apply(Vectors.scala:650)\r\n\tat org.apache.spark.ml.tree.CategoricalSplit.shouldGoLeft(Split.scala:99)\r\n\tat org.apache.spark.ml.tree.InternalNode.predictImpl(Node.scala:180)\r\n\tat org.apache.spark.ml.regression.RandomForestRegressionModel.$anonfun$predict$1(RandomForestRegressor.scala:259)\r\n\tat org.apache.spark.ml.regression.RandomForestRegressionModel.$anonfun$predict$1$adapted(RandomForestRegressor.scala:259)\r\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\r\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\r\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\r\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\r\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\r\n\tat org.apache.spark.ml.regression.RandomForestRegressionModel.predict(RandomForestRegressor.scala:259)\r\n\tat org.apache.spark.ml.regression.RandomForestRegressionModel.$anonfun$transform$1(RandomForestRegressor.scala:233)\r\n\tat org.apache.spark.ml.regression.RandomForestRegressionModel.$anonfun$transform$1$adapted(RandomForestRegressor.scala:233)\r\n\t... 17 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2403)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2352)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2351)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2351)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1109)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1109)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1109)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2591)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2533)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2522)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:898)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:476)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:429)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3715)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2728)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2728)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2935)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:287)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:326)\r\n\tat jdk.internal.reflect.GeneratedMethodAccessor71.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function (RandomForestRegressionModel$$Lambda$4544/0x00000001017d8840: (struct<type:tinyint,size:int,indices:array<int>,values:array<double>>) => double)\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:136)\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\nCaused by: java.lang.IndexOutOfBoundsException: Index 22 out of bounds [0, 21)\r\n\tat org.apache.spark.ml.linalg.SparseVector.apply(Vectors.scala:650)\r\n\tat org.apache.spark.ml.tree.CategoricalSplit.shouldGoLeft(Split.scala:99)\r\n\tat org.apache.spark.ml.tree.InternalNode.predictImpl(Node.scala:180)\r\n\tat org.apache.spark.ml.regression.RandomForestRegressionModel.$anonfun$predict$1(RandomForestRegressor.scala:259)\r\n\tat org.apache.spark.ml.regression.RandomForestRegressionModel.$anonfun$predict$1$adapted(RandomForestRegressor.scala:259)\r\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\r\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\r\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\r\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\r\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\r\n\tat org.apache.spark.ml.regression.RandomForestRegressionModel.predict(RandomForestRegressor.scala:259)\r\n\tat org.apache.spark.ml.regression.RandomForestRegressionModel.$anonfun$transform$1(RandomForestRegressor.scala:233)\r\n\tat org.apache.spark.ml.regression.RandomForestRegressionModel.$anonfun$transform$1$adapted(RandomForestRegressor.scala:233)\r\n\t... 17 more\r\n"
     ]
    }
   ],
   "source": [
    "pred.select(\"label\",\"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636c7d64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
